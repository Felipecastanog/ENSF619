# ğŸ“ prompt

This folder contains the finalized and structured **prompt** used to evaluate and compare Large Language Models (LLMs) â€” specifically **ChatGPT (GPT-4o)** and **DeepSeek-V3** â€” for automated code generation in privacy-preserving applications using **Differential Privacy (DP)**.

## ğŸ“„ Contents

- `prompt.md`:  
  The complete, step-by-step prompt designed to generate Python code that applies the Laplace mechanism to count records by birth decade in a synthetic dataset.

## ğŸ§© Purpose

The prompt in this folder is the result of an **iterative prompt engineering process**, which aimed to:
- Maximize code quality and reproducibility.
- Reduce ambiguity and variability in LLM outputs.
- Ensure strict adherence to Differential Privacy principles (Îµ = 0.1, sensitivity = 1).
- Generate **complete, executable Python scripts** directly from LLM responses.

The final prompt is structured as a **numbered list of explicit tasks**, which proved to be the most effective format for eliciting consistent and correct outputs from both LLMs.

## ğŸ›  Prompt Design Strategies Explored

During development, several strategies were evaluated and refined:
- âœ… **Structured step-by-step prompting** (most effective)  
- âŒ Goal-oriented prompting (too abstract)  
- âŒ Few-shot prompting (overfitting to example code)  
- âŒ Minimal prompting (ambiguous)  
- âŒ Narrative-style prompting (lack of determinism)

The selected prompt design ensures that LLMs receive **clear instructions**, including file paths, column names, DP parameters, and expected output formatting.

## ğŸš€ How to Use

You can copy and paste the contents of `final_structured_prompt.txt` into the interface of any general-purpose LLM with code generation capabilities (e.g., ChatGPT, DeepSeek). Modify file paths and column names as needed to adapt to new datasets.

---

This folder is essential for **reproducibility** and helps benchmark LLM behavior in response to a consistent technical query related to Differential Privacy.

